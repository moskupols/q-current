\input{../header.tex}

\begin{document}

\title{$\mbox{Теория вероятности}$}
\author{Фёдор Алексеев}
\raigorname{М.\,Е.\,Жуковский}

\maketitle
\gitlink\tableofcontents
%\newpage{}

\part*{Лекция 1. Вероятность. Вероятностное пространство. Алгебра}

\section{Базовые определения}

\begin{define*}
  $\Omega = \set{w1, \ldots, w2}$ --- \emph{множество элементарных событий} (\emph{элементарных исходов}).
\end{define*}

Вообще событий больше. Например, если монетку бросали два раза, то событие ``в первом случае выпал орёл'' происходит в двух из 
четырёх возможных исходов. Таким образом,

\begin{define*}
  Если $\Omega$ --- множество элементарных исходов, то множество $A \subset \Omega$ --- \emph{событие}.
\end{define*}

\begin{define*}[Временное]
  \emph{Вероятность} --- это функция $P: 2^{\Omega} \to [0, 1]$ такая, что:
  \begin{enumerate}
	\item $P(\Omega) = 1$
	\item $\forall A, B \subset \Omega, A \cup B = \varnothing \to P(A\cap B) = P(A) + P(B)$
  \end{enumerate}
\end{define*}

\begin{define*}[Временное]
  До поры до времени будем называть тройку $(\Omega, 2^{\Omega}, P)$ \emph{вероятностным пространством}, затем определение, например,
  вероятности несколько изменится.
\end{define*}

\begin{example}[Схема неупорядоченного выбора без повторений $k$ элементов из $n$] $ $ \\
  Пусть $k = 3, n = 10$. Тогда $\Omega = \set{w_1, \ldots, w_{C_{10}^{3}}}$. 
  Рассмотрим, например, событие $A = \set{w_1, \ldots, w_{C_{5}^{3}}}$, и положим $\forall i \le C_{10}^{3} \to P(w_i) = \frac{1}{C_{10}^{3}}$.

  $\forall	b \in 2^{\Omega} \to P(B) = \sum_{w\in B} P(w) = \frac{\abs{b}}{\abs{\Omega}}$. Тогда $P(A) = \frac{C_{5}^{3}}{C_{10}^{3}}$
\end{example}

А теперь рассмотрим произвольное (не обязательно конечное или даже счётное) множество элементарных событий. 
С ним всё не так-то просто, и для обращения с ним введём

\begin{define*}
  $\mathcal{A}$ --- система подмножеств $\Omega$ называеся \emph{алгеброй}, если выполняются следующие свойства:
  \begin{enumerate}
	\item $\Omega \in \mathcal{A}$
	\item $A \in \mathcal{A} \Rightarrow \overline{A} \in \mathcal{A} $
	\item $A, B \in \mathcal{A} \Rightarrow A \cap B \in \mathcal{A} $
  \end{enumerate}
\end{define*}

\begin{define*}
  Функция $P: \mathcal{A} \to \mathbb{R}$ --- \emph{конечно-аддитивная мера}, если верно следующее:
  \begin{enumerate}
	\item $\forall A \in \mathcal{A} \to P(A) \ge 0 $
	\item $\forall A, B \in \mathcal{A}: A \cap B = \varnothing \to P(A \cup B) = P(A) + P(B)$
  \end{enumerate}
\end{define*}

\begin{define*}
  Конечно-аддитивная мера со свойством $P(\Omega) < \infty$ --- \emph{конечно-аддитивная конечная мера}.
\end{define*}

\begin{define*}
  Если $P(\Omega) = 1$, то это \emph{конечно-аддитивная вероятностная мера} (\emph{конечно-аддитивная вероятность}).
\end{define*}

Однако свойства хочется усилить:
\begin{define*} Функция $P: \mathcal{A} \to \mathbb{R}$ \emph{называется счётно-аддитивной мерой}, если
  \begin{enumerate}
	\item $\forall A \in \mathcal{A}	\to P(A) \ge 0$
	\item $\forall A_1, A_2, \ldots \in \mathcal{A}: \forall i \neq j \to A_i \neq A_j, \bigcup_{i=1}^{\infty}A_i \in \mathcal{A}$, то 
	  $P(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^{\infty} P(A_i)$.
  \end{enumerate}
\end{define*}

\begin{define*}
  Если $P$ --- счётно-аддитивная мера и $P(\Omega) < \infty$, то $P$ --- счётно-аддитивная конечная мера.
  Если при этом $P(\Omega) = 1$, то $P$ --- \emph{вероятность}.
\end{define*}

\begin{define*}
  Если множество элементарных исходов $\Omega$ конечно, то $(\Omega, \mathcal{A}, P)$ --- \emph{вероятностное пространство в конечном случае}.

  Если $\Omega$ бесконечно, то $(\Omega, \mathcal{A}, P)$ ---  \emph{вероятностное пространство в общем случае}.
\end{define*}

\begin{define*}
  \emph{$\sigma$-алгеброй} на множестве $\Omega$ называется система его подмножеств $\mathcal{F}$ такая что
  \begin{enumerate}
	\item $\mathcal{F} $ --- алгебра.
	\item $\forall	A_1, A_2, \ldots \in \mathcal{F} \to \bigcap A_i \in \mathcal{F}$
  \end{enumerate}
\end{define*}

\section{Свойства алгебры и вероятностной меры}

\begin{remark*}[Свойства алгебры] $ $
  \begin{enumerate}
	\item $\varnothing \in \mathcal{A}$, так как $\varnothing = \overline{\Omega}$;
	\item $\forall A \cap B \in \mathcal{A}  \to  A \cup B = \overline{ \overline{A} \cap \overline{B} } \in \mathcal{A}$;
	\item $\forall A, B: A \setminus B \in \mathcal{A} \to A \setminus B = A \cap \overline{B} \in \mathcal{A} $;
	\item $\forall A_1, A_2, \ldots \in \mathcal{F}  \to \bigcup A_i \in \mathcal{F}, \bigcup A_i = \overline{\bigcap\overline{A_i}} \in \mathcal{F}$.
  \end{enumerate}
\end{remark*}

\begin{remark*}[Свойства вероятности] $ $
  \begin{enumerate}
	\item $P(\varnothing) = 0$, так как $P(\varnothing \cup \Omega) = P(\varnothing) + P(\Omega) = P(\varnothing) + 1$;
	\item $A \subset B \Rightarrow P(B) \ge P(A)$, так как $B = A \cup (B\setminus A)$;
	\item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, так как $P(A\cup B) = P(A) + P(B\setminus A)$, $P(B) = P(A \cap B) + P(B \setminus A)$.
  \end{enumerate}
\end{remark*}

\begin{Th}[О непрерывности вероятностной меры]
  Пусть $\Omega$ --- произвольное множество, $\mathcal{A} $ --- алгебра на нём. Тогда следующие свойства эквивалентны:
  \begin{enumerate}
	\item $P$ --- вероятность на $(\Omega, \mathcal{A})$
	\item $P$ --- конечно-аддитивная вероятность на $(\Omega, \mathcal{A})$, непрерывная сверху: 
	  \[
		\forall A_1, A_2, \ldots \in \mathcal{A}:  A_i \subseteq A_i+1 \mbox{ и } \bigcup A_i \in \mathcal{A} \to \lim\limits_{n\to\infty} P(A_n) = P(\bigcup A_i)
	  \]
	\item $P$ --- конечно-аддитивная вероятность на $(\Omega, \mathcal{A})$, непрерывная снизу:
	  \[
		\forall A_1, A_2, \ldots \in \mathcal{A}:  A_i \supseteq A_{i+1} \mbox{ и } \bigcap A_i \in \mathcal{A} \to \lim\limits_{n\to\infty} P(A_n) = 0
	  \]
	\item $P$ --- конечно-аддитивная вероятность на $(\Omega, \mathcal{A})$, непрерывная в нуле:
	  \[
		\forall A_1, \ldots \in \mathcal{A}: A_1 \supseteq A_2 \supseteq \ldots \mbox{ и } A_i \neq \varnothing \to \lim P(A_i) = 0
	  \]
  \end{enumerate}

  \begin{proof} $ $
	\begin{description}
	  \item [1 $\Rightarrow$ 2] Пусть $A_1, A_2, \ldots \in \mathcal{A}: A_1 \subseteq A_2 \subseteq \ldots$ и $\bigcup A_i \in \mathcal{A} $. 
		
		Рассмотрим $B_1 = A_1, B_2 = A_2\setminus A_1, B_3 = A_3 \setminus A_2, \ldots$. 
		
		Тогда $\bigcup A_i = \bigcup B_i$, при этом $\forall i \neq j \to B_i \cap B_j = \varnothing$. Тогда $P(\bigcup A_i) = P(\bigcup B_i) = 
		\sum P(B_i) = \sum (P(A_i) - P(A_{i-1})) = \lim\limits_{n\to\infty} \sum\limits_{i=2}^{n} (P(A_i) - P(A_{i-1})) = \lim\limits_{n\to\infty} P(A_n)$

	  \item [2 $\Rightarrow$ 3] Пусть $A_i, A_2, \ldots \in \mathcal{A}: A_1 \supseteq A_2 \supseteq \ldots \mbox{ и } A = \bigcap A_i \in \mathcal{A}$.

		Тогда $P(A) = 1 - P(\overline{A}) = 1 - P(\bigcup \overline{A_i}) = 1 - \lim\limits_{n\to\infty} P(\overline{A_n}) = 
		1 - \lim\limits_{n\to\infty}(1 - P(A_n)) = 1 - 1 + \lim\limits_{n\to\infty} P(A_n)$.

	  \item [3 $\Rightarrow$ 4] Очевидно.

	  \item [4 $\Rightarrow$ 1] Пусть $A_1, A_2, \ldots \in \mathcal{A}: A_i \cap A_j = \varnothing \mbox{ и } \bigcup A_i \in \mathcal{A}$.
		
		Рассмотрим множество $B_n = A \setminus (A_1 \cup \ldots \cup A_n)$. Ясно, что $B_1 \supseteq B_2 \supseteq \ldots$. 
		
		Кроме того, $\bigcap B_i = \varnothing$: пусть $w \in \bigcap B_i$, тогда $\exists n : w \in A_n$, однако $w \not\in B_n$!
		
		Тогда по свойству 4 
		\begin{multline*}
		  \lim\limits_{n\to\infty} P(B_n) = 0 = \lim\limits_{n\to\infty} P(A\setminus (A_1 \cup \ldots \cup A_n)) = \\
		  \lim\limits_{n\to\infty} (P(A) - P(A_1 \cup \ldots \cup A_n)) = 
		  P(A) - \lim\limits_{n\to\infty} P(A_1 \cup \ldots \cup A_n) = \\
		  P(A) - \lim\limits_{n\to\infty} \sum\limits_{i=1}^{n} P(A_i) = P(A) - \sum\limits_{i=1}^{n} P(A_i) \Rightarrow \mbox{$P$ --- вероятность.}
		  \qedhere{}
		\end{multline*}
		
	\end{description}
  \end{proof}
\end{Th}

\begin{example}
  Классическая вероятность --- это когда $\Omega$ конечна, $\mathcal{A} = 2^{\Omega}, P(A) = \frac{\abs{A}}{\abs{\Omega}}$
\end{example}

\begin{example}
  Геометрическая вероятность.

  Пусть $\Omega \subset \mathbb{R} ^n$, и у него есть объём $\mu$. Тогда $\mathcal{A} $ --- все подмножества $\Omega$, у которых есть объём.
  Тогда $P(A) = \frac{\mu(A)}{\mu(\Omega)}$.
\end{example}

\begin{example}[Задача о встрече]
  Пусть есть два студента, у них часовой перерыв, они хотят встретиться. Они договорились, что если один приходит и ждёт более 15 минут, то уходит.
  Приходят они в случайный момент времени в пределах перерыва. Какова вероятность, что встретятся?

  Давайте нарисуем две оси, обозначающие время прихода каждого из студентов. Ясно, что встречаются они в некоторой полосе между $y = x - \frac14$
  и $y = x + \frac14$. 
  
  Тогда вероятность встречи $P(A) = \frac{\mu(A)}{\mu(\Omega)} = \frac{7}{16}$.
\end{example}


\part*{Лекция 2. Условные вероятности. Независимость событий}

\section{Условная вероятность}

Предположим, у нас есть полка, на ней три книги: Толстого, Пушкина, Горького. Вероятность наугад вытянуть Пушкина --- $\frac13$ .
Но если вы уверены, что не возьмёте Горького, то уже $\frac12$. Как это посчитать? $\frac12 = \frac{\frac13}{\frac23}$.

\begin{define*}
  Пусть $(\Omega, \mathcal{F}, P)$ --- вероятностное пространство, $A, B \in \mathcal{F} $. Тогда \emph{условной вероятностью} $P(A|B)$ 
  называется $\frac{P(A\cap B)}{P(B)}$ (0, если $P(B) = 0$).
\end{define*}

\begin{example}
  Вспомним задачу про студентов. Пусть событие $A$ --- факт встречи, 
  событие $B$ --- ``хотя бы один из студентов пришёл позже 15 минут от начала отсчёта''. 
  Получается, при условии $B$ на прошлом графике мы как бы вырезаем квадратик в углу. Тогда
  $P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{\mu(A\cap B)}{\mu(B)} = \frac{1 - 9/16 - 1/16}{1 - 1/16} = \frac{1}{5}$.
\end{example}

\begin{claim}
  Если $B \in \mathcal{F} $ --- событие: $P(B) > 0$, то $P(\cdot |B)$ --- вероятностная мера на $(\Omega, \mathcal{F} )$.

  \begin{proof}
	\begin{enumerate}
	  \item $P(\Omega|B) = \frac{P(\Omega \cup B)}{P(B)} = 1$
	  \item Пусть $A_1, A_2, \ldots \in \mathcal{F}, \forall i \neq j \to A_i \cap A_j =  \varnothing $

		\[
		  P\left(\bigcup A_i | B\right) = \frac{P\bigl((\bigcup A_i) \cup B\bigr)}{P(B)} 
		  = \frac{P\bigl(\bigcup (A_i \cap B)\bigr)}{P(B)} = \frac{\sum P(A_i \cap B)}{B}
		  = \sum \frac{P(A_i \cap B)}{B} = \sum P(A_i | B)
		\]
	\end{enumerate}
  \end{proof}
\end{claim}

\begin{claim}[Формула полной вероятности]
  Пусть $(\Omega, \mathcal{F}, P)$ --- вероятностное пространство, $D_1, D_2, \ldots \in \mathcal{F}, \forall i \neq j \to D_i \cap D_j = \varnothing$, 
  $A \in \mathcal{F} $.
  Тогда $P(A) = \sum_{i=1}^\infty P(A | D_i)P(D_i)$

  \begin{proof}
	$A = \bigcup_{i=1}^\infty (D_i \cap A)$, при этом $(D_i \cap A) \cap (D_j \cap A) = \varnothing$.

	Следовательно, $P(A) = \sum P(A\cap D_i), P(A|D_i) = \frac{P(A\cap D_i)}{P(D_i)}$ при $P(D_i \neq 0) \Rightarrow 
	P(A\cap D_i) = P(A|D_i)P(D_i)$.

	Итак, $P(A) = \sum_{i=1}^\infty P(A|D_i)P(D_i)$.
  \end{proof}
\end{claim}

\begin{remark*}
  Разбиение не обязано быть счётным, может быть и конечным.
\end{remark*}

\begin{example}
  Есть $n$ шаров, из них $k$ чёрных и $n-k$ белых. Мы по очереди достаём шары, не возвращая обратно. Какова вероятность в $j$-й раз вытянуть чёрных шар?

  Какова, для начала, вероятность в первый раз вытянуть чёрный? $\frac{k}{n}$. Утверждается, что эта вероятность не изменится. 
  Приведём доказательство тремя способами.

  \begin{proof}
	\begin{enumerate}
	  \item 
		Занумеруем шары: $\set{1,\ldots,n}$. $\Omega = \set{(i_1,\ldots,i_j) \mbox{ --- различные числа из } \set{1,\ldots n}}$.
		Пусть $A = \set{(1,i_2,\ldots,i_j)}$, то есть событие, когда первым мы вытянули шар номер 1, и $B = \set{(i_1,\ldots,i_{j-1},1)}$. 
		Проведя несложную биекцию, можно показать, что в $A$ и $B$ одинаковое число элементов.

		Пусть первые $k$ номеров чёрные, тогда
		\begin{gather*}
		  P(\mbox{\Small при $j$-том выборе вытянут чёрный шар}) 
		  = P(\mbox{\Small при $j$-том выборе выбран шар $i \le k$}) =\\
		  = \sum_{i=1}^k P(\mbox{\Small при первом выборе выбран шар $i$})
		  = P(\mbox{\Small первый раз выбран чёрный шар}) = \frac{k}{n}.
		\end{gather*}

	  \item (В лоб формулой полной вероятности). 
		Пусть $D_i$ --- событие ``После $(j-1)$-ого выбора осталось $i$ чёрных шаров.'' Пусть $A_j$ --- интересующее нас событие.

		\begin{gather*}
		  P(A_j) = \sum_{i=\max(k-j+1,0)}^{\min(k, n-j-1)} P(A_j | D_i)P(D_i) = \\
		  \sum \frac{i}{n - j + 1} \frac{C_{j-1}^{k-i} \cdot k(k-1)\cdots(k-(k-i)+1)}{(n-k)(n-k-1)\cdots(n-k-(j-1)+1)} = 
		  \frac{C_{n-1}^{k-1}}{C_{n}^{k}} = \frac{k}{n}
		\end{gather*}

	  \item (По индукции).
		Пусть $D_0$ --- событие ``При первом вытягивании выбран чёрный'', $D_1$ --- событие ``При первом вытягивании выбран белый''.

		Мы знаем, что $P(A_1) = \frac{k}{n}$. Предположим, что $P(A_{i-1}) = \frac{k}{n}$, и докажем переход:
		\begin{gather*}
		  P(A_j) = P(A_j | D_0)P(D_0) + P(A_j | D_1)P(D_1) = \\ = \frac{k-1}{n-1}\frac{k}{n} + \frac{k-1}{n-1}\frac{n-k}{n}
		  = \frac{k^2 - k + kn - k^2}{n(n-1)} = \frac{k(n-1)}{n(n-1)} = \frac{k}{n}.
		\end{gather*}
	\end{enumerate}
  \end{proof}
\end{example}

Перейдём к формуле, которую удобно использовать, исходя из того, что произошло в итоге.

\begin{claim}[Формула Байеса]
  Пусть $(\Omega, \mathcal{F}, P)$ --- вероятностное пространство, $\Omega = D_1 \cup D_2 \cup \ldots, A \in \mathcal{F}, P(A) > 0$. Тогда
  \[
	P(D_n|A) = \frac{P(D_n \cap A)}{P(A)} = \frac{P(A|D_n)P(D_n)}{\sum_k^{\infty} P(A|D_n)P(D_k)}.
  \]
\end{claim}

\begin{example}
  Снова вспомним про студентов. Такая интерпретация: сторонний наблюдатель увидел, что они встретились.
  Какова вероятность, что они встретились позже, чем в первые 15 минут?
  
  $D_0$ --- ``оба пришли в первые 15 минут'', $D_1 = \Omega \setminus D_0$.
  $P(D_1|A) = \frac{P(A|D_1)P(D_1)}{P(A|D_0)P(D_0) + P(A|D_1)P(D_1)} = 
  \frac{\frac1{5}\cdot\left(1-\frac1{16}\right)}{1\cdot\frac1{16} + \frac{2}{5}\left(1-\frac{1}{16}\right)} = \frac{6}{7}$.
\end{example}

\section{Независимость событий}

Говоря житейским языком, события независимы, если одно не зависти от другого, т.е. условная вероятность не зависит от условия.

\begin{define*}
  События $A, B \in \mathcal{F} $ называются \emph{независимыми}, если $P(A\cap B) = P(A)P(B)$.
\end{define*}

\begin{remark*}
  Определение становится естественным, если расписать:
  $P(A) = \frac{P(A\cap B)}{P(A)} = P(A|B)$
\end{remark*}

\begin{define*}
  События $A_1, \ldots, A_n$ называются \emph{независимыми в совокупности}, если $\forall k_1,\ldots,k_j \in \set{1,\ldots,n}, k_q \neq k_p \to 
  P(\bigcap A_{k_j}) = \prod P(A_{k_j})$.

  События называются \emph{попарно независимыми}, если $\forall i \neq j \to P(A_i\cap A_j) = P(A_i)P(A_j)$.
\end{define*}

\begin{remark*}
  Из независимой совокупности следует попарная, обратное неверно.
\end{remark*}

\begin{example}
  Пусть у нас есть тетраэдр, у которого три грани покрашены каждая в свой цвет, а четвёртая --- во все три. 
  Тогда события вида ``после броска мы увидели цвет $x$'' попарно независимы, но не в совокупности.

  $\Omega = \set{w_1, w_2, w_3, w_4}, A= \set{w_1, w_4}, B = \set{w_2, w_4}, C = \set{w_3,w_4}$

  $P(A\cap B) = P(\set{w_4}) = \frac{1}{4} = \frac{1}{2} \cdot \frac{1}{2} = P(A)P(B)$ --- попарная независимость.
  
  $P(A\cap B\cap C) = P(\set{w_4}) = \frac{1}{4} \neq (\frac{1}{2})^3$
\end{example}

\begin{define*}
  События $\set{A_\gamma | \gamma \in \Gamma}$ \emph{независимы в совокупности}, если для любых различных $\gamma_1, \ldots, \gamma_n \in \Gamma$
  $P(\bigcap A_{\gamma_i}) = \prod P(A_{\gamma_i})$.

  Пусть $M_1, \ldots, M_n$ --- системы событий. Они \emph{независимы в совокупности}, если $\forall A_i \in M_i$ независимы в совокупности.
\end{define*}

\begin{example}
  Пусть $A_1,\ldots,A_n$ --- независимы в совокупности. Тогда $\mathcal{F}_{A_i}$ --- независимы в совокупности. 
  ($\mathcal{F}_A = \set{\varnothing, A, \overline{A}, \Omega}$).

  \begin{proof}
	Пусть $B_i \in \mathcal{F}_i, i_1,\cdots,i_k \in \set{1,\cdots,n}$.

	Если $B_{i_j} \neq \varnothing$, то $P(\bigcap B_{i_j}) = 0 = \prod P(B_{i_j})$.

	Если $B_{i_j} = \Omega$, то $P(\bigcap B_{i_r}) = P(\bigcap_{r\neq j} B_{i_r}) = \prod_{r\neq j}P(B_{i_r})$.

	Если отрицаний 0, то всё доказано. Пусть доказано для $j$ отрицаний, докажем для $j+1$.

	Пусть $B_{i_1} = \overline{M_1}$

	$P\left(\bigcap B_{i_r}\right) = P\left(\overline{A_{i_1}} \cap \bigcap_{r>1}B_{i_r}\right) 
	= P\left(\bigcap B_{i_r}\right) - P\left(\overline{A_{i_1}} \cap \bigcap_{r>1}B_{i_r}\right) 
	= \prod_{r>1} P(B_{i_r}) \cdot (1-P(A_{i_1})) = \prod P(B_i)$.
  \end{proof}
\end{example}

\end{document}

